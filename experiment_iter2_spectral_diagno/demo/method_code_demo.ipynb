{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8viky6buyz",
   "metadata": {},
   "source": [
    "# Spectral Diagnostics: Walk Resolution Limit Hypothesis\n",
    "\n",
    "This notebook demonstrates **comprehensive spectral diagnostic analysis** validating four key aspects of the walk resolution limit hypothesis across multiple graph datasets:\n",
    "\n",
    "1. **Spectral Sparsity Validation** -- Are local spectral measures sparse (few dominant eigenvalues per node)?\n",
    "2. **SRI Distribution Analysis** -- How do Spectral Resolution Index distributions differ across datasets?\n",
    "3. **Node-Level vs Graph-Level Resolution** -- Does eigenvector localization improve resolution?\n",
    "4. **Vandermonde Conditioning** -- Does condition number predict reconstruction error?\n",
    "\n",
    "Plus a baseline eigenvalue clustering analysis for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pdm4q9aytdj",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:16.919323Z",
     "iopub.status.busy": "2026-02-22T19:06:16.919235Z",
     "iopub.status.idle": "2026-02-22T19:06:27.031622Z",
     "shell.execute_reply": "2026-02-22T19:06:27.031331Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import subprocess, sys\n",
    "\n",
    "def _pip(*args):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *args])\n",
    "\n",
    "# --- Colab pre-installed: pin to Colab versions if on Colab ---\n",
    "_COLAB_PINS = {\n",
    "    \"numpy\": \"numpy==1.26.4\",\n",
    "    \"scipy\": \"scipy==1.13.1\",\n",
    "    \"matplotlib\": \"matplotlib==3.7.1\",\n",
    "    \"seaborn\": \"seaborn==0.13.2\",\n",
    "}\n",
    "\n",
    "import importlib, os\n",
    "_on_colab = \"google.colab\" in sys.modules\n",
    "for _pkg, _pin in _COLAB_PINS.items():\n",
    "    if _on_colab:\n",
    "        pass  # already installed on Colab\n",
    "    else:\n",
    "        _pip(_pin)\n",
    "\n",
    "# --- Non-Colab packages (always install) ---\n",
    "_pip(\"loguru\")\n",
    "\n",
    "print(\"All dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dpxhlf21t0l",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:27.032834Z",
     "iopub.status.busy": "2026-02-22T19:06:27.032722Z",
     "iopub.status.idle": "2026-02-22T19:06:27.550413Z",
     "shell.execute_reply": "2026-02-22T19:06:27.549939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "# Plot defaults\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.labelsize': 14,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11,\n",
    "    'legend.fontsize': 11,\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 150,\n",
    "    'savefig.bbox': 'tight',\n",
    "})\n",
    "\n",
    "print(\"Imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "can04m3ns1s",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:27.551487Z",
     "iopub.status.busy": "2026-02-22T19:06:27.551382Z",
     "iopub.status.idle": "2026-02-22T19:06:27.553526Z",
     "shell.execute_reply": "2026-02-22T19:06:27.553288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading helper defined.\n"
     ]
    }
   ],
   "source": [
    "GITHUB_DATA_URL = \"https://raw.githubusercontent.com/AMGrobelnik/ai-invention-ace67e-the-walk-resolution-limit-a-super-resolu/main/experiment_iter2_spectral_diagno/demo/mini_demo_data.json\"\n",
    "\n",
    "def load_data():\n",
    "    try:\n",
    "        import urllib.request\n",
    "        with urllib.request.urlopen(GITHUB_DATA_URL) as response:\n",
    "            return json.loads(response.read().decode())\n",
    "    except Exception:\n",
    "        pass\n",
    "    if os.path.exists(\"mini_demo_data.json\"):\n",
    "        with open(\"mini_demo_data.json\") as f:\n",
    "            return json.load(f)\n",
    "    raise FileNotFoundError(\"Could not load mini_demo_data.json\")\n",
    "\n",
    "print(\"Data loading helper defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdm2rpljzf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:27.554276Z",
     "iopub.status.busy": "2026-02-22T19:06:27.554208Z",
     "iopub.status.idle": "2026-02-22T19:06:27.778720Z",
     "shell.execute_reply": "2026-02-22T19:06:27.778224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 83 graphs across 4 datasets:\n",
      "  Synthetic-aliased-pairs: 33 graphs\n",
      "  ZINC-subset: 20 graphs\n",
      "  Peptides-func: 15 graphs\n",
      "  Peptides-struct: 15 graphs\n"
     ]
    }
   ],
   "source": [
    "raw_data = load_data()\n",
    "datasets = raw_data['datasets']\n",
    "\n",
    "total_graphs = sum(len(gs) for gs in datasets.values())\n",
    "print(f\"Loaded {total_graphs} graphs across {len(datasets)} datasets:\")\n",
    "for ds_name, graphs in datasets.items():\n",
    "    print(f\"  {ds_name}: {len(graphs)} graphs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1pcruwu2",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Tunable parameters for all analyses. Adjust these to control runtime vs. detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "n8mxqd86h89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:27.780430Z",
     "iopub.status.busy": "2026-02-22T19:06:27.780334Z",
     "iopub.status.idle": "2026-02-22T19:06:27.782557Z",
     "shell.execute_reply": "2026-02-22T19:06:27.782308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: K_VALUES=[2, 4, 8, 16, 20], N_SUBSAMPLE=200, NOISE_LEVELS=[1e-08, 1e-06, 0.0001, 0.01], NODES_PER_GRAPH=5\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG: All tunable parameters ===\n",
    "\n",
    "# Walk lengths K to evaluate SRI and Vandermonde conditioning\n",
    "K_VALUES = [2, 4, 8, 16, 20]\n",
    "K_KEYS = [f'K={k}' for k in K_VALUES]\n",
    "\n",
    "# Weight thresholds for node-level resolution analysis (Analysis 3)\n",
    "WEIGHT_THRESHOLDS = [0.01, 0.05, 0.10]\n",
    "\n",
    "# Vandermonde reconstruction experiment (Analysis 4)\n",
    "N_SUBSAMPLE = 200  # Original: 200 (all graphs fit easily with demo data)\n",
    "NOISE_LEVELS = [1e-8, 1e-6, 1e-4, 1e-2]  # Original: 4 noise levels\n",
    "MAX_COND = 1e15\n",
    "NODES_PER_GRAPH = 5  # Original: 5 nodes per graph for reconstruction\n",
    "\n",
    "print(f\"Config: K_VALUES={K_VALUES}, N_SUBSAMPLE={N_SUBSAMPLE}, \"\n",
    "      f\"NOISE_LEVELS={NOISE_LEVELS}, NODES_PER_GRAPH={NODES_PER_GRAPH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70sm3q4hwj",
   "metadata": {},
   "source": [
    "## Analysis 1: Spectral Sparsity Validation\n",
    "\n",
    "For each graph, we examine how many eigenvalues contribute significantly to each node's local spectral measure. If few eigenvalues dominate (low effective rank), spectral measures are **sparse** -- confirming a key assumption of the walk resolution limit hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22n7wd053ih",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:27.783387Z",
     "iopub.status.busy": "2026-02-22T19:06:27.783314Z",
     "iopub.status.idle": "2026-02-22T19:06:27.794647Z",
     "shell.execute_reply": "2026-02-22T19:06:27.794392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis 1: Spectral Sparsity Validation ===\n",
      "  Synthetic-aliased-pairs: median_eff_rank=8.0, median_n=10, ratio=0.8000, confirmed=False\n",
      "  ZINC-subset: median_eff_rank=10.0, median_n=22, ratio=0.4651, confirmed=False\n",
      "  Peptides-func: median_eff_rank=10.0, median_n=134, ratio=0.0746, confirmed=True\n",
      "  Peptides-struct: median_eff_rank=10.0, median_n=134, ratio=0.0746, confirmed=True\n",
      "  Sparsity analysis complete in 0.0s\n"
     ]
    }
   ],
   "source": [
    "def run_sparsity_analysis(datasets):\n",
    "    \"\"\"Analysis 1: Spectral Sparsity Validation.\"\"\"\n",
    "    print(\"=== Analysis 1: Spectral Sparsity Validation ===\")\n",
    "    t0 = time.time()\n",
    "    sparsity_results = {}\n",
    "    sparsity_cache = {}\n",
    "\n",
    "    for ds_name, graphs in datasets.items():\n",
    "        all_eff_rank_1pct = []\n",
    "        all_eff_rank_5pct = []\n",
    "        all_participation_ratios = []\n",
    "        all_spectral_entropies = []\n",
    "        all_n_eigenvalues = []\n",
    "\n",
    "        for g in graphs:\n",
    "            n_eigenvalues = len(g['eigenvalues'])\n",
    "            all_n_eigenvalues.append(n_eigenvalues)\n",
    "            for node_measures in g['local_spectral']:\n",
    "                if not node_measures or len(node_measures) == 0:\n",
    "                    continue\n",
    "                weights = np.array([m[1] for m in node_measures], dtype=np.float64)\n",
    "                total_w = weights.sum()\n",
    "                if total_w < 1e-12:\n",
    "                    continue\n",
    "\n",
    "                eff_1 = int(np.sum(weights > 0.01 * total_w))\n",
    "                all_eff_rank_1pct.append(eff_1)\n",
    "\n",
    "                eff_5 = int(np.sum(weights > 0.05 * total_w))\n",
    "                all_eff_rank_5pct.append(eff_5)\n",
    "\n",
    "                pr = float(total_w ** 2 / np.sum(weights ** 2))\n",
    "                all_participation_ratios.append(pr)\n",
    "\n",
    "                p = weights / total_w\n",
    "                p = p[p > 0]\n",
    "                entropy = float(-np.sum(p * np.log2(p)))\n",
    "                all_spectral_entropies.append(entropy)\n",
    "\n",
    "        if len(all_eff_rank_1pct) == 0:\n",
    "            print(f\"  No valid nodes found for {ds_name}, skipping\")\n",
    "            continue\n",
    "\n",
    "        arr_1 = np.array(all_eff_rank_1pct)\n",
    "        arr_5 = np.array(all_eff_rank_5pct)\n",
    "        arr_pr = np.array(all_participation_ratios)\n",
    "        arr_n = np.array(all_n_eigenvalues)\n",
    "\n",
    "        median_eff = float(np.median(arr_1))\n",
    "        median_n = float(np.median(arr_n))\n",
    "        ratio = median_eff / max(median_n, 1e-10)\n",
    "\n",
    "        sparsity_cache[ds_name] = {\n",
    "            'eff_rank_1pct': arr_1,\n",
    "            'eff_rank_5pct': arr_5,\n",
    "            'participation_ratios': arr_pr,\n",
    "        }\n",
    "\n",
    "        sparsity_results[ds_name] = {\n",
    "            'median_eff_rank_1pct': median_eff,\n",
    "            'median_graph_size': median_n,\n",
    "            'sparsity_ratio': ratio,\n",
    "            'sparsity_confirmed': bool(ratio < 0.3),\n",
    "            'n_nodes_analyzed': len(all_eff_rank_1pct),\n",
    "        }\n",
    "        print(f\"  {ds_name}: median_eff_rank={median_eff:.1f}, \"\n",
    "              f\"median_n={median_n:.0f}, ratio={ratio:.4f}, \"\n",
    "              f\"confirmed={ratio < 0.3}\")\n",
    "\n",
    "    print(f\"  Sparsity analysis complete in {time.time()-t0:.1f}s\")\n",
    "    return sparsity_results, sparsity_cache\n",
    "\n",
    "sparsity_results, sparsity_cache = run_sparsity_analysis(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8wi5a74b3bb",
   "metadata": {},
   "source": [
    "## Analysis 2: SRI Distribution Analysis\n",
    "\n",
    "The **Spectral Resolution Index (SRI)** measures how well random walks of length K can distinguish neighboring eigenvalues. We compare SRI distributions across datasets using Kolmogorov-Smirnov tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tqgo761j98d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:27.795627Z",
     "iopub.status.busy": "2026-02-22T19:06:27.795556Z",
     "iopub.status.idle": "2026-02-22T19:06:27.836825Z",
     "shell.execute_reply": "2026-02-22T19:06:27.836317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis 2: SRI Distribution Analysis ===\n",
      "  Synthetic-aliased-pairs: SRI(K=20) mean=18.9830, pct_below_1=0.0%\n",
      "  ZINC-subset: SRI(K=20) mean=1.3766, pct_below_1=40.0%\n",
      "  Peptides-func: SRI(K=20) mean=0.0348, pct_below_1=100.0%\n",
      "  Peptides-struct: SRI(K=20) mean=0.0348, pct_below_1=100.0%\n",
      "  KS(Peptides-func vs Peptides-struct): stat=0.0000, p=1.00e+00\n",
      "  KS(Peptides-func vs Synthetic-aliased-pairs): stat=1.0000, p=1.83e-12\n",
      "  KS(Peptides-func vs ZINC-subset): stat=1.0000, p=6.16e-10\n",
      "  KS(Peptides-struct vs Synthetic-aliased-pairs): stat=1.0000, p=1.83e-12\n",
      "  KS(Peptides-struct vs ZINC-subset): stat=1.0000, p=6.16e-10\n",
      "  KS(Synthetic-aliased-pairs vs ZINC-subset): stat=0.8485, p=1.10e-09\n",
      "  SRI analysis complete in 0.0s\n"
     ]
    }
   ],
   "source": [
    "def run_sri_analysis(datasets):\n",
    "    \"\"\"Analysis 2: SRI Distribution Analysis with KS tests.\"\"\"\n",
    "    print(\"=== Analysis 2: SRI Distribution Analysis ===\")\n",
    "    t0 = time.time()\n",
    "    sri_results = {}\n",
    "    sri_by_dataset = {}\n",
    "\n",
    "    for ds_name, graphs in datasets.items():\n",
    "        delta_mins = np.array([g['delta_min'] for g in graphs], dtype=np.float64)\n",
    "        sri_20 = np.array([g['sri']['K=20'] for g in graphs], dtype=np.float64)\n",
    "\n",
    "        spectral_ranges = np.array([\n",
    "            max(g['eigenvalues']) - min(g['eigenvalues']) for g in graphs\n",
    "        ], dtype=np.float64)\n",
    "        normalized_sri = delta_mins / np.maximum(spectral_ranges, 1e-10)\n",
    "\n",
    "        sri_by_K = {}\n",
    "        for k_key in K_KEYS:\n",
    "            sri_by_K[k_key] = np.array([g['sri'][k_key] for g in graphs], dtype=np.float64)\n",
    "\n",
    "        sri_by_dataset[ds_name] = {\n",
    "            'delta_min': delta_mins,\n",
    "            'sri_20': sri_20,\n",
    "            'normalized_sri': normalized_sri,\n",
    "            'spectral_range': spectral_ranges,\n",
    "            'sri_by_K': sri_by_K,\n",
    "        }\n",
    "\n",
    "        sri_results[ds_name] = {\n",
    "            'delta_min_mean': float(np.mean(delta_mins)),\n",
    "            'sri_20_mean': float(np.mean(sri_20)),\n",
    "            'sri_20_median': float(np.median(sri_20)),\n",
    "            'pct_below_1': float(np.mean(sri_20 < 1.0) * 100),\n",
    "            'pct_above_5': float(np.mean(sri_20 > 5.0) * 100),\n",
    "        }\n",
    "        print(f\"  {ds_name}: SRI(K=20) mean={np.mean(sri_20):.4f}, \"\n",
    "              f\"pct_below_1={np.mean(sri_20 < 1.0)*100:.1f}%\")\n",
    "\n",
    "    # Pairwise KS tests\n",
    "    dataset_names = sorted(sri_by_dataset.keys())\n",
    "    ks_results = {}\n",
    "    for i, ds1 in enumerate(dataset_names):\n",
    "        for ds2 in dataset_names[i + 1:]:\n",
    "            stat_sri, p_sri = stats.ks_2samp(\n",
    "                sri_by_dataset[ds1]['sri_20'],\n",
    "                sri_by_dataset[ds2]['sri_20']\n",
    "            )\n",
    "            ks_results[f\"{ds1}_vs_{ds2}\"] = {\n",
    "                'statistic': float(stat_sri),\n",
    "                'p_value': float(p_sri),\n",
    "            }\n",
    "            print(f\"  KS({ds1} vs {ds2}): stat={stat_sri:.4f}, p={p_sri:.2e}\")\n",
    "\n",
    "    sri_results['ks_tests'] = ks_results\n",
    "    print(f\"  SRI analysis complete in {time.time()-t0:.1f}s\")\n",
    "    return sri_results, sri_by_dataset\n",
    "\n",
    "sri_results, sri_by_dataset = run_sri_analysis(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sl5309o29l",
   "metadata": {},
   "source": [
    "## Analysis 3: Node-Level vs Graph-Level Resolution\n",
    "\n",
    "Eigenvector localization means each node \"sees\" only a subset of eigenvalues. This can improve spectral resolution at the node level compared to the graph level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "xo9fdl17w6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:27.838004Z",
     "iopub.status.busy": "2026-02-22T19:06:27.837863Z",
     "iopub.status.idle": "2026-02-22T19:06:27.869605Z",
     "shell.execute_reply": "2026-02-22T19:06:27.869044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis 3: Node-Level vs Graph-Level Resolution ===\n",
      "  Synthetic-aliased-pairs thresh=0.01: median_ratio=1.00, pct_better=39.4%\n",
      "  Synthetic-aliased-pairs thresh=0.05: median_ratio=1.00, pct_better=42.4%\n",
      "  Synthetic-aliased-pairs thresh=0.1: median_ratio=1.00, pct_better=51.5%\n",
      "  ZINC-subset thresh=0.01: median_ratio=1.94, pct_better=100.0%\n",
      "  ZINC-subset thresh=0.05: median_ratio=1.94, pct_better=100.0%\n",
      "  ZINC-subset thresh=0.1: median_ratio=2.16, pct_better=100.0%\n",
      "  Peptides-func thresh=0.01: median_ratio=61.88, pct_better=100.0%\n",
      "  Peptides-func thresh=0.05: median_ratio=61.88, pct_better=100.0%\n",
      "  Peptides-func thresh=0.1: median_ratio=64.33, pct_better=100.0%\n",
      "  Peptides-struct thresh=0.01: median_ratio=61.88, pct_better=100.0%\n",
      "  Peptides-struct thresh=0.05: median_ratio=61.88, pct_better=100.0%\n",
      "  Peptides-struct thresh=0.1: median_ratio=64.33, pct_better=100.0%\n",
      "  Node vs Graph analysis complete in 0.0s\n"
     ]
    }
   ],
   "source": [
    "def _compute_node_deltas_for_graph(g, thresh):\n",
    "    \"\"\"Compute node-level delta_min values for a single graph.\"\"\"\n",
    "    graph_delta_min = g['delta_min']\n",
    "    if graph_delta_min < 1e-15:\n",
    "        return [], graph_delta_min\n",
    "\n",
    "    node_deltas = []\n",
    "    for node_measures in g['local_spectral']:\n",
    "        if not node_measures or len(node_measures) == 0:\n",
    "            continue\n",
    "        eigenvals = np.array([m[0] for m in node_measures], dtype=np.float64)\n",
    "        weights = np.array([m[1] for m in node_measures], dtype=np.float64)\n",
    "        if len(weights) == 0 or weights.max() < 1e-12:\n",
    "            continue\n",
    "        mask = weights > thresh * weights.max()\n",
    "        sig_eigenvals = eigenvals[mask]\n",
    "        if len(sig_eigenvals) < 2:\n",
    "            continue\n",
    "        sig_sorted = np.sort(sig_eigenvals)\n",
    "        diffs = np.diff(sig_sorted)\n",
    "        nonzero_diffs = diffs[diffs > 1e-15]\n",
    "        if len(nonzero_diffs) > 0:\n",
    "            node_deltas.append(float(np.min(nonzero_diffs)))\n",
    "\n",
    "    return node_deltas, graph_delta_min\n",
    "\n",
    "\n",
    "def run_node_vs_graph_analysis(datasets):\n",
    "    \"\"\"Analysis 3: Node-level vs Graph-level resolution comparison.\"\"\"\n",
    "    print(\"=== Analysis 3: Node-Level vs Graph-Level Resolution ===\")\n",
    "    t0 = time.time()\n",
    "    node_vs_graph_results = {}\n",
    "    scatter_cache = {}\n",
    "\n",
    "    for ds_name, graphs in datasets.items():\n",
    "        node_vs_graph_results[ds_name] = {}\n",
    "        scatter_cache[ds_name] = {}\n",
    "\n",
    "        for thresh in WEIGHT_THRESHOLDS:\n",
    "            ratios = []\n",
    "            node_sris = []\n",
    "            graph_sris = []\n",
    "\n",
    "            for g in graphs:\n",
    "                node_deltas, graph_delta_min = _compute_node_deltas_for_graph(g, thresh)\n",
    "                if not node_deltas or graph_delta_min < 1e-15:\n",
    "                    continue\n",
    "                median_node_delta = float(np.median(node_deltas))\n",
    "                if median_node_delta > 0:\n",
    "                    ratio = median_node_delta / graph_delta_min\n",
    "                    ratios.append(ratio)\n",
    "                    node_sris.append(median_node_delta * 20)\n",
    "                    graph_sris.append(graph_delta_min * 20)\n",
    "\n",
    "            scatter_cache[ds_name][f'{thresh}'] = {\n",
    "                'node_sris': node_sris,\n",
    "                'graph_sris': graph_sris,\n",
    "                'ratios': ratios,\n",
    "            }\n",
    "\n",
    "            result = {'n_graphs_analyzed': len(ratios)}\n",
    "            if ratios:\n",
    "                ratios_arr = np.array(ratios)\n",
    "                result.update({\n",
    "                    'mean_ratio': float(np.mean(ratios_arr)),\n",
    "                    'median_ratio': float(np.median(ratios_arr)),\n",
    "                    'pct_node_better': float(np.mean(ratios_arr > 1.0) * 100),\n",
    "                })\n",
    "                if len(graph_sris) > 2:\n",
    "                    corr = stats.spearmanr(graph_sris, node_sris)\n",
    "                    result['spearman_corr'] = float(corr.statistic)\n",
    "            else:\n",
    "                result.update({\n",
    "                    'mean_ratio': None,\n",
    "                    'median_ratio': None,\n",
    "                    'pct_node_better': None,\n",
    "                })\n",
    "\n",
    "            node_vs_graph_results[ds_name][f'threshold_{thresh}'] = result\n",
    "            if ratios:\n",
    "                print(f\"  {ds_name} thresh={thresh}: \"\n",
    "                      f\"median_ratio={np.median(ratios):.2f}, \"\n",
    "                      f\"pct_better={np.mean(np.array(ratios) > 1)*100:.1f}%\")\n",
    "\n",
    "    print(f\"  Node vs Graph analysis complete in {time.time()-t0:.1f}s\")\n",
    "    return node_vs_graph_results, scatter_cache\n",
    "\n",
    "node_vs_graph_results, scatter_cache = run_node_vs_graph_analysis(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727mcpa7tro",
   "metadata": {},
   "source": [
    "## Analysis 4: Vandermonde Condition Number Analysis\n",
    "\n",
    "The Vandermonde matrix condition number controls how noise in random walk statistics amplifies into reconstruction error. We verify that higher condition numbers (from closer eigenvalues) lead to larger errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bhkj0blvf1v",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:27.871003Z",
     "iopub.status.busy": "2026-02-22T19:06:27.870870Z",
     "iopub.status.idle": "2026-02-22T19:06:27.936146Z",
     "shell.execute_reply": "2026-02-22T19:06:27.935791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis 4: Vandermonde Conditioning Analysis ===\n",
      "  Processing Synthetic-aliased-pairs (33 graphs)...\n",
      "    Growth rate: mean=0.6710\n",
      "    Reconstruction: 163 samples\n",
      "  Processing ZINC-subset (20 graphs)...\n",
      "    Growth rate: mean=0.6491\n",
      "    Reconstruction: 100 samples\n",
      "  Processing Peptides-func (15 graphs)...\n",
      "    Growth rate: mean=0.5359\n",
      "    Reconstruction: 75 samples\n",
      "  Processing Peptides-struct (15 graphs)...\n",
      "    Growth rate: mean=0.5359\n",
      "    Reconstruction: 75 samples\n",
      "  Vandermonde analysis complete in 0.1s\n"
     ]
    }
   ],
   "source": [
    "def run_vandermonde_analysis(datasets):\n",
    "    \"\"\"Analysis 4: Vandermonde Condition Number Analysis.\"\"\"\n",
    "    print(\"=== Analysis 4: Vandermonde Conditioning Analysis ===\")\n",
    "    t0 = time.time()\n",
    "    vander_results = {}\n",
    "    reconstruction_results = {}\n",
    "    vander_scatter_cache = {}\n",
    "\n",
    "    for ds_name, graphs in datasets.items():\n",
    "        print(f\"  Processing {ds_name} ({len(graphs)} graphs)...\")\n",
    "\n",
    "        # (a) Growth rate analysis\n",
    "        growth_rates = []\n",
    "        sri_values_K20 = []\n",
    "        all_log_conds = {k: [] for k in K_VALUES}\n",
    "\n",
    "        for g in graphs:\n",
    "            conds = [g['vandermonde_cond'][kk] for kk in K_KEYS]\n",
    "            log_conds = np.log10(np.clip(np.array(conds, dtype=np.float64), 1.0, MAX_COND))\n",
    "            slope, _, _, _, _ = stats.linregress(K_VALUES, log_conds)\n",
    "            growth_rates.append(float(slope))\n",
    "            sri_values_K20.append(g['sri']['K=20'])\n",
    "            for ki, kv in enumerate(K_VALUES):\n",
    "                all_log_conds[kv].append(float(log_conds[ki]))\n",
    "\n",
    "        growth_rates_arr = np.array(growth_rates)\n",
    "        sri_values_arr = np.array(sri_values_K20)\n",
    "\n",
    "        # (b) Correlation: growth rate vs SRI\n",
    "        corr_growth_sri = None\n",
    "        if len(growth_rates_arr) > 2:\n",
    "            try:\n",
    "                corr = stats.spearmanr(growth_rates_arr, sri_values_arr)\n",
    "                corr_growth_sri = {\n",
    "                    'spearman_rho': float(corr.statistic),\n",
    "                    'p_value': float(corr.pvalue),\n",
    "                }\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        vander_results[ds_name] = {\n",
    "            'growth_rate_mean': float(np.mean(growth_rates_arr)),\n",
    "            'growth_rate_median': float(np.median(growth_rates_arr)),\n",
    "            'corr_growth_rate_vs_sri': corr_growth_sri,\n",
    "            'cond_number_stats': {},\n",
    "            'sri_quartile_conds': {},\n",
    "        }\n",
    "\n",
    "        for kv in K_VALUES:\n",
    "            log_conds_arr = np.array(all_log_conds[kv])\n",
    "            vander_results[ds_name]['cond_number_stats'][str(kv)] = {\n",
    "                'median_log10': float(np.median(log_conds_arr)),\n",
    "            }\n",
    "\n",
    "        # SRI quartile analysis\n",
    "        if len(sri_values_arr) >= 4:\n",
    "            quartiles = np.percentile(sri_values_arr, [25, 50, 75])\n",
    "            bins = [-np.inf] + list(quartiles) + [np.inf]\n",
    "            for kv in K_VALUES:\n",
    "                lc = np.array(all_log_conds[kv])\n",
    "                q_means = []\n",
    "                for qi in range(4):\n",
    "                    mask = (sri_values_arr >= bins[qi]) & (sri_values_arr < bins[qi + 1])\n",
    "                    if mask.sum() > 0:\n",
    "                        q_means.append(float(np.mean(lc[mask])))\n",
    "                    else:\n",
    "                        q_means.append(0.0)\n",
    "                vander_results[ds_name]['sri_quartile_conds'][str(kv)] = q_means\n",
    "\n",
    "        print(f\"    Growth rate: mean={np.mean(growth_rates_arr):.4f}\")\n",
    "\n",
    "        # (c) RWSE Reconstruction Error Experiment\n",
    "        np.random.seed(42)\n",
    "        n_sample = min(N_SUBSAMPLE, len(graphs))\n",
    "        sample_indices = np.random.choice(len(graphs), n_sample, replace=False)\n",
    "\n",
    "        errors_by_noise = {eps: [] for eps in NOISE_LEVELS}\n",
    "        cond_nums_recon = []\n",
    "        recon_scatter_conds = []\n",
    "        recon_scatter_errs = []\n",
    "\n",
    "        for idx in sample_indices:\n",
    "            g = graphs[idx]\n",
    "            local_spectral = g['local_spectral']\n",
    "            for node_idx in range(min(NODES_PER_GRAPH, len(local_spectral))):\n",
    "                measure = local_spectral[node_idx]\n",
    "                if not measure or len(measure) < 2:\n",
    "                    continue\n",
    "\n",
    "                eigs = np.array([m[0] for m in measure], dtype=np.float64)\n",
    "                weights_true = np.array([m[1] for m in measure], dtype=np.float64)\n",
    "                if len(eigs) < 2 or np.linalg.norm(weights_true) < 1e-12:\n",
    "                    continue\n",
    "\n",
    "                K = 20\n",
    "                V = np.array([[eig ** (k + 1) for eig in eigs] for k in range(K)],\n",
    "                             dtype=np.float64)\n",
    "\n",
    "                try:\n",
    "                    cond = float(np.linalg.cond(V))\n",
    "                except Exception:\n",
    "                    cond = MAX_COND\n",
    "                if not np.isfinite(cond):\n",
    "                    cond = MAX_COND\n",
    "                cond = min(cond, MAX_COND)\n",
    "                cond_nums_recon.append(cond)\n",
    "\n",
    "                m_true = V @ weights_true\n",
    "\n",
    "                for eps in NOISE_LEVELS:\n",
    "                    noise = eps * np.random.randn(K)\n",
    "                    m_noisy = m_true + noise\n",
    "                    try:\n",
    "                        w_hat, _, _, _ = np.linalg.lstsq(V, m_noisy, rcond=None)\n",
    "                        rel_error = float(np.linalg.norm(w_hat - weights_true) /\n",
    "                                          max(np.linalg.norm(weights_true), 1e-12))\n",
    "                        if not np.isfinite(rel_error):\n",
    "                            rel_error = 1e10\n",
    "                        errors_by_noise[eps].append(min(rel_error, 1e10))\n",
    "                    except Exception:\n",
    "                        errors_by_noise[eps].append(1e10)\n",
    "\n",
    "                    if eps == NOISE_LEVELS[-2] if len(NOISE_LEVELS) >= 2 else eps == NOISE_LEVELS[0]:\n",
    "                        recon_scatter_conds.append(np.log10(max(cond, 1.0)))\n",
    "                        recon_scatter_errs.append(np.log10(max(errors_by_noise[eps][-1], 1e-15)))\n",
    "\n",
    "        vander_scatter_cache[ds_name] = {\n",
    "            'conds': recon_scatter_conds,\n",
    "            'errors': recon_scatter_errs,\n",
    "        }\n",
    "\n",
    "        reconstruction_results[ds_name] = {}\n",
    "        for eps in NOISE_LEVELS:\n",
    "            errs = np.array(errors_by_noise[eps])\n",
    "            if len(errs) > 0:\n",
    "                reconstruction_results[ds_name][str(eps)] = {\n",
    "                    'mean_error': float(np.mean(errs)),\n",
    "                    'median_error': float(np.median(errs)),\n",
    "                    'n_samples': len(errs),\n",
    "                }\n",
    "\n",
    "        # (d) Correlation: cond vs error\n",
    "        ref_eps = NOISE_LEVELS[-2] if len(NOISE_LEVELS) >= 2 else NOISE_LEVELS[0]\n",
    "        if len(cond_nums_recon) > 2 and len(errors_by_noise[ref_eps]) > 2:\n",
    "            min_len = min(len(cond_nums_recon), len(errors_by_noise[ref_eps]))\n",
    "            log_conds_r = np.log10(np.maximum(cond_nums_recon[:min_len], 1.0))\n",
    "            log_errors_r = np.log10(np.maximum(errors_by_noise[ref_eps][:min_len], 1e-15))\n",
    "            valid = np.isfinite(log_conds_r) & np.isfinite(log_errors_r)\n",
    "            if valid.sum() > 2:\n",
    "                try:\n",
    "                    corr = stats.spearmanr(log_conds_r[valid], log_errors_r[valid])\n",
    "                    reconstruction_results[ds_name]['corr_cond_vs_error'] = {\n",
    "                        'spearman_rho': float(corr.statistic),\n",
    "                        'p_value': float(corr.pvalue),\n",
    "                    }\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        print(f\"    Reconstruction: {len(cond_nums_recon)} samples\")\n",
    "\n",
    "    print(f\"  Vandermonde analysis complete in {time.time()-t0:.1f}s\")\n",
    "    return vander_results, reconstruction_results, vander_scatter_cache\n",
    "\n",
    "vander_results, reconstruction_results, vander_scatter_cache = run_vandermonde_analysis(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207c01lb92l",
   "metadata": {},
   "source": [
    "## Analysis 5: Eigenvalue Clustering Baseline\n",
    "\n",
    "A simple baseline that counts distinct eigenvalue clusters and measures spectral gaps, providing context for the SRI-based diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "w5wynqk2gol",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:27.937494Z",
     "iopub.status.busy": "2026-02-22T19:06:27.937422Z",
     "iopub.status.idle": "2026-02-22T19:06:27.941931Z",
     "shell.execute_reply": "2026-02-22T19:06:27.941704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Analysis 5: Eigenvalue Clustering Baseline ===\n",
      "  Synthetic-aliased-pairs: median_clusters=7.0\n",
      "  ZINC-subset: median_clusters=21.0\n",
      "  Peptides-func: median_clusters=107.0\n",
      "  Peptides-struct: median_clusters=107.0\n",
      "  Baseline analysis complete in 0.0s\n"
     ]
    }
   ],
   "source": [
    "def run_eigenvalue_clustering_baseline(datasets):\n",
    "    \"\"\"Baseline analysis: eigenvalue clustering coefficient.\"\"\"\n",
    "    print(\"=== Analysis 5: Eigenvalue Clustering Baseline ===\")\n",
    "    t0 = time.time()\n",
    "    baseline_results = {}\n",
    "\n",
    "    for ds_name, graphs in datasets.items():\n",
    "        n_clusters_list = []\n",
    "        spectral_gaps = []\n",
    "        max_gaps = []\n",
    "\n",
    "        for g in graphs:\n",
    "            eigs = np.sort(g['eigenvalues'])\n",
    "            n = len(eigs)\n",
    "            if n < 2:\n",
    "                continue\n",
    "\n",
    "            clusters = 1\n",
    "            for i in range(1, n):\n",
    "                if abs(eigs[i] - eigs[i - 1]) > 0.01:\n",
    "                    clusters += 1\n",
    "            n_clusters_list.append(clusters)\n",
    "\n",
    "            diffs = np.diff(eigs)\n",
    "            nonzero = diffs[diffs > 1e-10]\n",
    "            if len(nonzero) > 0:\n",
    "                spectral_gaps.append(float(nonzero[0]))\n",
    "                max_gaps.append(float(np.max(nonzero)))\n",
    "            else:\n",
    "                spectral_gaps.append(0.0)\n",
    "                max_gaps.append(0.0)\n",
    "\n",
    "        baseline_results[ds_name] = {\n",
    "            'median_clusters': float(np.median(n_clusters_list)) if n_clusters_list else None,\n",
    "            'median_spectral_gap': float(np.median(spectral_gaps)) if spectral_gaps else None,\n",
    "            'median_max_gap': float(np.median(max_gaps)) if max_gaps else None,\n",
    "            'n_graphs': len(n_clusters_list),\n",
    "        }\n",
    "        print(f\"  {ds_name}: median_clusters={baseline_results[ds_name]['median_clusters']}\")\n",
    "\n",
    "    print(f\"  Baseline analysis complete in {time.time()-t0:.1f}s\")\n",
    "    return baseline_results\n",
    "\n",
    "baseline_results = run_eigenvalue_clustering_baseline(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heaf1p2r1lh",
   "metadata": {},
   "source": [
    "## Results Summary and Visualization\n",
    "\n",
    "Key findings displayed as a summary table and plots showing SRI distributions, node-level resolution benefits, and Vandermonde conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vjhxnzfr8ho",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-22T19:06:27.943005Z",
     "iopub.status.busy": "2026-02-22T19:06:27.942940Z",
     "iopub.status.idle": "2026-02-22T19:06:28.167879Z",
     "shell.execute_reply": "2026-02-22T19:06:28.167504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SPECTRAL DIAGNOSTICS SUMMARY\n",
      "================================================================================\n",
      "Dataset                   Sparsity     SRI<1 %    Node>Graph %   Clusters  \n",
      "--------------------------------------------------------------------------------\n",
      "Peptides-func             YES          100.0%     100.0%         107       \n",
      "Peptides-struct           YES          100.0%     100.0%         107       \n",
      "Synthetic-aliased-pairs   NO           0.0%       42.4%          7         \n",
      "ZINC-subset               NO           40.0%      100.0%         21        \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualization complete.\n"
     ]
    }
   ],
   "source": [
    "# === Summary Table ===\n",
    "print(\"=\" * 80)\n",
    "print(\"SPECTRAL DIAGNOSTICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "header = f\"{'Dataset':<25} {'Sparsity':<12} {'SRI<1 %':<10} {'Node>Graph %':<14} {'Clusters':<10}\"\n",
    "print(header)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for ds_name in sorted(datasets.keys()):\n",
    "    sp = sparsity_results.get(ds_name, {})\n",
    "    sr = sri_results.get(ds_name, {})\n",
    "    ng = node_vs_graph_results.get(ds_name, {}).get('threshold_0.05', {})\n",
    "    bl = baseline_results.get(ds_name, {})\n",
    "\n",
    "    sparsity_str = \"YES\" if sp.get('sparsity_confirmed') else \"NO\"\n",
    "    sri_pct = f\"{sr.get('pct_below_1', 0):.1f}%\"\n",
    "    node_pct = f\"{ng.get('pct_node_better', 0):.1f}%\" if ng.get('pct_node_better') is not None else \"N/A\"\n",
    "    clusters = f\"{bl.get('median_clusters', 0):.0f}\" if bl.get('median_clusters') is not None else \"N/A\"\n",
    "\n",
    "    print(f\"{ds_name:<25} {sparsity_str:<12} {sri_pct:<10} {node_pct:<14} {clusters:<10}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# === Visualization ===\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "ds_names = sorted(datasets.keys())\n",
    "colors = sns.color_palette(\"husl\", len(ds_names))\n",
    "\n",
    "# Plot 1: SRI Distribution (log scale)\n",
    "ax = axes[0, 0]\n",
    "for i, ds in enumerate(ds_names):\n",
    "    vals = sri_by_dataset[ds]['sri_20']\n",
    "    vals_pos = vals[vals > 0]\n",
    "    if len(vals_pos) > 0:\n",
    "        ax.hist(np.log10(vals_pos), bins=20, alpha=0.5, label=ds, color=colors[i])\n",
    "ax.axvline(x=0, color='red', linestyle='--', alpha=0.7, label='SRI=1')\n",
    "ax.set_xlabel('log10(SRI at K=20)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('SRI Distribution (K=20)')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Plot 2: SRI vs K\n",
    "ax = axes[0, 1]\n",
    "for i, ds_name in enumerate(ds_names):\n",
    "    graphs = datasets[ds_name]\n",
    "    median_sri = []\n",
    "    for k_key in K_KEYS:\n",
    "        sri_vals = [g['sri'][k_key] for g in graphs]\n",
    "        median_sri.append(float(np.median(sri_vals)))\n",
    "    ax.plot(K_VALUES, median_sri, '-o', label=ds_name, color=colors[i])\n",
    "ax.axhline(y=1.0, color='red', linestyle='--', label='SRI=1 (resolution limit)')\n",
    "ax.set_xlabel('Walk Length K')\n",
    "ax.set_ylabel('Median SRI')\n",
    "ax.set_title('SRI vs Walk Length K')\n",
    "ax.set_yscale('log')\n",
    "ax.legend(fontsize=8)\n",
    "\n",
    "# Plot 3: Sparsity - Effective Rank\n",
    "ax = axes[1, 0]\n",
    "data_1 = [sparsity_cache[ds]['eff_rank_1pct'] for ds in ds_names if ds in sparsity_cache]\n",
    "labels = [ds for ds in ds_names if ds in sparsity_cache]\n",
    "if data_1:\n",
    "    vp = ax.violinplot(data_1, positions=range(len(labels)), showmeans=True, showmedians=True)\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_xticklabels([n.replace('-', '\\n') for n in labels], fontsize=8)\n",
    "    ax.axhline(y=10, color='red', linestyle='--', alpha=0.7, label='Top-10 ceiling')\n",
    "    ax.set_ylabel('Effective Rank')\n",
    "    ax.set_title('Effective Rank (1% threshold)')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "# Plot 4: Vandermonde Condition Number Heatmap\n",
    "ax = axes[1, 1]\n",
    "data_mat = []\n",
    "ylabels = []\n",
    "for ds in ds_names:\n",
    "    row = []\n",
    "    for kv in K_VALUES:\n",
    "        cs = vander_results.get(ds, {}).get('cond_number_stats', {}).get(str(kv), {})\n",
    "        row.append(cs.get('median_log10', 0))\n",
    "    data_mat.append(row)\n",
    "    ylabels.append(ds)\n",
    "if data_mat:\n",
    "    data_arr = np.array(data_mat)\n",
    "    sns.heatmap(data_arr, annot=True, fmt='.1f',\n",
    "                xticklabels=[f'K={k}' for k in K_VALUES],\n",
    "                yticklabels=ylabels, cmap='YlOrRd', ax=ax)\n",
    "    ax.set_title('Median Vandermonde Cond. Number (log10)')\n",
    "\n",
    "fig.suptitle('Spectral Diagnostics Overview', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"\\nVisualization complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
